# Stepâ€‘Audioâ€‘EditX Multiâ€‘Voice Cloner Node ğŸ™ï¸

This project is a custom node implementation built on top of Step-Audio-EditX. It adapts and extends EditX capabilities to support **multiâ€‘speaker**, **longâ€‘format**, **voice cloning**, and **emotion/style/speed editing**, enabling you to feed in a script with multiple speakers, inline pauses, paralinguistic cues (like laughter, breathing), and get a concatenated audio output â€” all in one pass.

---

## Description

The original Stepâ€‘Audioâ€‘EditX model enables singleâ€‘speaker voice cloning and emotion/style editing given a reference prompt audio + text.  

This node extends that capability, allowing you to:

- Provide multiple â€œspeakerâ€ reference voices at once.  
- Write a simple script with speaker tags, inline pauses, and optionally emotion/style/speed tags.  
- Generate a **single contiguous audio file** with all voices, pauses, and editing applied.  
- Handle paralinguistic markers (like `[Laughter]`, `[Breathing]`, etc.) â€” these are preserved and synthesis attempts to reflect them as natural speech or silence, depending on your engineâ€™s capabilities.  

In short: you can build multiâ€‘voice dialogues, audio stories, podcasts, or voiceâ€‘over sequences in one go.

---

## Features

- Multiâ€‘speaker support (map each speaker to a reference audio + prompt).  
- Inline speaker switching via `[speakerX]` tags.  
- Inline pauses via `[pause]N]` syntax (pause of N milliseconds).  
- Emotion / style / speed tags (e.g. `[happy]`, `[serious]`, `[faster]`) for each line.  
- Paralinguistic tag support â€” e.g. `[Laughter]`, `[Breathing]`, `[Sigh]`, `[Dissatisfaction-hnn]`, etc. Those tags remain in the output text.  
- Automatic concatenation of generated audio segments into one final waveform.  
- Progress reporting (with progress bar).  
- Graceful handling of missing speakerâ€‘tags (defaults to first speaker).  

---

## How It Works

1. Parse the input script line by line.  
2. Detect tags:
   - `[speakerX]` â€” which reference voice/prompt to use.  
   - Optional leading tags like emotion, style, speed (e.g. `[happy]`, `[whisper]`, `[slower]`).  
   - Paralinguistic tags (preserved).  
   - `[pause]` tags â€” interpreted as â€œgenerate silence for N ms.â€  
3. For each speech line, call `clone_from_tensor(...)` (and optionally repeated editing for emotion / style / speed).  
4. For pause lines, generate a tensor of zeros of the requested duration.  
5. Collect all segments (speech or silence), concatenate them, and return a single audio output.  

---

## Usage

Example usage in a ComfyUI flow:

```text
[speaker1][happy]Hello there!  
[pause]500  
[speaker2][sad][whisper]Iâ€™m not sure about thisâ€¦  
[speaker1][Laughter]Thatâ€™s hilarious!  
```

- Provide reference audios & prompts for each speaker.  
- Feed this script to the node.  
- Get a single AUDIO output: concatenated waveform with cloned voices, pauses, and editing.  

---

## Installation / Integration
```bash
   cd custom_nodes
   git clone https://github.com/vantagewithai/Vantage-Step-Audio-EditX.git
   cd Vantage-Step-Audio-EditX
   pip install -r requirements.txt
   ```  
4. Launch ComfyUI â€” the node should appear under category **`Vantage/Step-Audio-EditX`**.  

## Download Models
[Download](https://huggingface.co/vantagewithai/Step-Fun-EditX-ComfyUI)

After downloading the models, copy them into ComfyUI/models, you should have the following structure:
```
ComfyUI/
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ Step-Audio-EditX/
â”‚   â”œâ”€â”€â”€â”€ CosyVoice-300M-25Hz/
â”‚   â”‚     â”œâ”€â”€â”€ campplus.onnx
â”‚   â”‚     â”œâ”€â”€â”€ cosyvoice.yaml
â”‚   â”‚     â”œâ”€â”€â”€ flow.pt
â”‚   â”‚     â””â”€â”€â”€ hift.pt
â”‚   â”œâ”€â”€â”€â”€ dengcunqin/
â”‚   â”œâ”€â”€â”€â”€ â””â”€â”€â”€ speech_paraformer-large_asr_nat-zh-cantonese-en-16k-vocab8501-online/
â”‚   â”‚          â”œâ”€â”€â”€ am.mvn
â”‚   â”‚          â”œâ”€â”€â”€ config.yaml
â”‚   â”‚          â”œâ”€â”€â”€ configuration.json
â”‚   â”‚          â”œâ”€â”€â”€ model.pt
â”‚   â”‚          â”œâ”€â”€â”€ seg_dict
â”‚   â”‚          â”œâ”€â”€â”€ tokens.json
â”‚   â”‚          â”œâ”€â”€â”€ tokens.txt
â”‚   â”‚          â””â”€â”€â”€ write_tokens_from_txt.py
â”‚   â”œâ”€â”€ model.safetensors
â”‚   â””â”€â”€ speech_tokenizer_v1.onnx
```
---

## Script Syntax

| Tag Type | Syntax | Meaning |
|---------|--------|---------|
| Speaker switch | `[speakerX]` | Use speaker number X (1-based) |
| Pause / silence | `[pause]300` | Insert 300 ms of silence |
| Emotion | `[happy]`, `[sad]`, â€¦ | First valid emotion tag per line is applied |
| Style | `[whisper]`, `[serious]`, â€¦ | First valid style tag per line is applied |
| Speed modifier | `[faster]`, `[slower]`, â€¦ | First valid speed tag per line is applied |
| Paralinguistic cue | `[Laughter]`, `[Breathing]`, `[Sigh]`, â€¦ | Preserved in the text â€” not stripped. May be used for downstream effects. |

Tags **must** come before the actual text of the line (after `[speakerX]`).  

Example:

```
[speaker2][happy][whisper][slower]I am fine!  
[speaker1][Laughter]That was funny!  
[pause]500  
```

---

## Limitations & Notes

- The quality of voice cloning / emotion/style editing depends on the underlying `Stepâ€‘Audioâ€‘EditX` engine and your reference audio & prompt.  
- Paralinguistic tags are preserved in the text passed to the engine â€” if the engine doesnâ€™t support them, they may just render as silence or be ignored.  
- If sample rates of speakers vary, the node currently assumes uniform sample rate in the concatenation step.  
- Long scripts may consume significant VRAM / memory â€” monitor usage accordingly.  
- The node does **not** perform grammar or punctuation correction â€” script should be wellâ€‘formatted.  

---

## License & Credits

**License: MIT**

This project builds upon the original **Stepâ€‘Audioâ€‘EditX** repository (see [https://github.com/stepfun-ai/Step-Audio-EditX](https://github.com/stepfun-ai/Step-Audio-EditX)).  

Please refer to the original repository for the base license.

